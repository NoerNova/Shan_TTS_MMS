{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install --upgrade pip \n",
    "%pip install datasets[audio]\n",
    "%pip install evaluate\n",
    "%pip install transformers\n",
    "%pip install jiwer\n",
    "%pip install accelerate\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "shn_dataset_train = load_dataset(\"norhsangpha/shn-asr-datasets\", split=\"train\", token=True)\n",
    "shn_dataset_test = load_dataset(\"norhsangpha/shn-asr-datasets\", split=\"test\", token=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "shn_dataset_train = load_from_disk(\"../../Datasets/mms-asr-nova-datasets-500/train\")\n",
    "shn_dataset_test = load_from_disk(\"../../Datasets/mms-asr-nova-datasets-500/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 422\n",
      "})\n",
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 105\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(shn_dataset_train)\n",
    "print(shn_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·Å∂·Äù·Ä∫·Åµ·Ä±·ÇÉ·Çà ·Ä¢·ÄΩ·Åº·Ä∫·Åµ·Åº·Ä∫·Ä¢·ÄΩ·Åµ·Ä∫·Çá·Äû·ÄØ·Äô·Ä∫·Çâ·Äô·ÇÉ·Ä∏·Äê·Ä∞·Å∫·Ä∫·Ä∏·Äû·Ä± ·Äú·ÇÜ·Çà·ÇÅ·Åº·Ä∫·ÅΩ·Ä∞·Çà·Äô·Å¢·ÇÜ·Åº·Åº·Ä∫·Çâ ·Åº·ÄΩ·Åº·Ä∫·Ä∏·Äú·Ä∞·Äô·Ä∫·Çâ·Äê·Å¢·ÇÜ·Åµ·ÇÇ·ÇÉ·Çá·Äö·Äù·Ä∫·Çâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>·Å∂·ÇÉ·Çà·Äê·Ä±·Äû·Ä≠·ÄØ·Äï·Ä∫·Çá·Ä¢·Äù·Ä∫·Äï·ÄØ·Åº·Ä∫·Çà·ÅΩ·ÄΩ·Åº·Ä∫·Ä∏·Åµ·ÇÇ·ÇÉ·Çá·Åµ·Ä∞·Å∫·Ä∫·Ä∏ ·Äú·Ä∞·Äù·Ä∫·Çá·ÇÅ·Äµ·Äê·Ä∫·Ä∏·Äû·ÄÑ·Ä∫·Åµ·Ä±·ÇÉ·Çà·Ä¢·ÄØ·Äï·Ä∫·Çá·Åµ·Åº·Ä∫·Äê·ÄÑ·Ä∫·Ä∏·Äù·Åº·Ä∫·Ä∏·Å∏·Ä≠·ÄØ·Åº·Ä∫·Çà·Åµ·ÇÇ·ÇÉ·Çá·Äú·ÇÑ·Çà·Åº·ÇÉ·Çà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·Äû·Äô·Ä∫·Çâ·Äù·ÇÉ·Çà·Äú·ÄΩ·ÄÑ·Ä∫·Çà·Äô·Åº·Ä∫·Ä∏·Äï·Ä∞·Åº·Ä∫·Çâ·Äô·ÇÉ·Ä∏·ÇÅ·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·Å¢·Åº·Ä∫·Çá·ÇÅ·Ä≠·ÄØ·Äù·Ä∫·Åµ·Ä±·ÇÉ·Çà ·Äú·ÄΩ·ÄÑ·Ä∫·Çà·Äù·Åº·Ä∫·Ä∏·Äú·ÄÑ·Ä∫·Åº·Åº·Ä∫·Çâ ·Äö·ÄÑ·Ä∫·Ä∏·Äê·Ä≠·ÄØ·Åµ·Ä∫·Çâ·Ä¢·Äù·Ä∫·Äô·ÇÉ·Ä∏·Äú·Å¢·Äê·Ä∫·Çà·Äë·Ä≠·ÄØ·ÄÑ·Ä∫·Åµ·Åº·Ä∫·Äú·ÇÜ·Çà·Äö·Ä∞·Çá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·Äù·Åº·Ä∫·Ä∏·Å∏·Ä≠·ÄØ·Åº·Ä∫·Çà·Äö·Åµ·Ä∫·Çâ·Äê·Ä∞·Å∫·Ä∫·Ä∏·Åº·ÇÉ·Çà·Å∏·Å¢·ÇÜ·Ä∏·Äú·Ä≠·Ä∞·Åº·Ä∫·Äö·Äù·Ä∫·Çâ ·Äë·Ä∞·Å∫·Ä∫·Çà·Å∏·ÇÇ·Ä∫·Ä¢·ÄΩ·Åµ·Ä∫·Çá·Äô·ÇÉ·Ä∏·Åº·ÄÑ·Ä∫·Çá·Äú·Ä∞·Äô·Ä∫·Å∏·ÇÇ·Ä∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>·Åµ·Ä∞·Åº·Ä∫·Ä∏·Åº·ÇÜ·Çâ ·Äô·Åº·Ä∫·Ä∏·Äê·Ä±·Äô·ÄÆ·Ä∏·ÅΩ·ÇÇ·Ä∫·Ä¢·Åº·Ä∫·ÇÅ·Åº·Ä∫·ÄÑ·Ä≠·ÄØ·Åº·Ä∫·Ä∏·Äö·Äù·Ä∫·Çâ ·Äê·Ä±·Äë·Äµ·ÄÑ·Ä∫·Äú·ÇÜ·Çà ·Åº·ÇÇ·Ä∫·Ä∏·Åº·Åº·Ä∫·Çâ·Åµ·Ä±·ÇÉ·Çà·Äö·ÄÑ·Ä∫·Ä∏·Äï·ÇÉ·Ä∏·Åµ·Ä∞·Åº·Ä∫·Ä∏·Å∏·Ä≠·Ä∞·ÄÑ·Ä∫·Çâ·Åº·ÄÑ·Ä∫·Çá·Å∏·Äù·Ä∫·Çà·Äô·Ä≠·Ä∞·ÄÑ·Ä∫·Ä∏·Å∂·Äô·Ä∫·Ä∏·Äö·Ä∞·Çá·Äú·Ä∞·Ä∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>·Äù·Åº·Ä∫·Ä∏·Å∏·Ä≠·ÄØ·Åº·Ä∫·Çà·Ä¢·Äù·Ä∫·Äú·ÇÖ·Åº·Ä∫·Ä∏·Äö·ÇÉ·Çà·Äô·Åº·Ä∫·Ä∏·Äë·ÇÖ·ÄÑ·Ä∫·Çà·Äú·ÇÖ·Åº·Ä∫·Ä∏·Åº·Ä≠·ÄØ·ÄÑ·Ä∫·Çà ·Å∏·Äï·Ä∫·Ä∏·Åæ·ÇÜ·Ä∏·Äö·Äù·Ä∫·Çâ ·Äú·ÄØ·Äê·Ä∫·Çá·Åµ·ÇÇ·ÇÉ·Çá·Åµ·Äô·Ä∫·Ä∏·Åº·Ä≠·ÄØ·ÄÑ·Ä∫·Çà·Äû·Ä±·Äú·Å¢·Äê·Ä∫·Çà·Äë·ÇÖ·ÄÑ·Ä∫·Çà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>·Äô·Ä≠·Ä∞·Äù·Ä∫·Çà·Äï·Ä≠·Ä∞·Åº·Ä∫·Çà·Åµ·ÇÇ·ÇÉ·Çá·Äô·Ä∞·Äê·Ä∫·Ä∏·Äö·Äù·Ä∫·Çâ ·Å∏·Äù·Ä∫·Çà·Äú·ÄΩ·Åº·Ä∫·Çâ·Å∂·Ä≠·Ä∞·Äù·Ä∫·Ä∏·Åº·Åº·Ä∫·Çâ·Åµ·Ä±·ÇÉ·Çà ·Äô·ÇÜ·Çà·Å∏·ÇÇ·Ä∫·Äô·ÇÜ·Çà·Å∂·Ä±·ÇÉ·Ä∏ ·Äû·Äô·Ä∫·Çâ·Äï·Ä±·ÇÉ·Ä∏·Åµ·ÄΩ·Äê·Ä∫·Çá·Å∂·Äô·Ä∫·Ä∏·Äú·Ä≠·ÄØ·Åº·Ä∫·Ä∏·Å∂·Äù·Ä∫·Çà·Äö·Ä∞·Çá·Åº·ÇÇ·Ä∫·Ä∏·Ä¢·Ä∞·Åµ·Ä∫·Ä∏·Äô·Åº·Ä∫·Ä∏·Åµ·ÇÇ·ÇÉ·Çá·Äö·Äù·Ä∫·Çâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>·Äï·Ä±·ÇÉ·Ä∏·Äù·ÇÉ·Çà·Äï·Äµ·Åº·Ä∫·Åµ·Å¢·Åº·Ä∫·Äú·ÇÜ·Çà·ÄÑ·Ä≠·ÄØ·Åº·Ä∫·Ä∏ ·Äû·Äô·Ä∫·Çâ·Äù·ÇÉ·Çà·Äô·Åº·Ä∫·Ä∏·Äö·Å¢·Äï·Ä∫·Çá·Åµ·Ä±·ÇÉ·Çà ·Åµ·ÇÉ·Çà·Äê·Ä±·Äú·ÇÜ·Çà·Å∏·ÄØ·Åº·Ä∫·Çâ·ÇÅ·Ä±·ÇÅ·Äµ·Äê·Ä∫·Ä∏·Ä¢·Ä≠·Ä∞·Äù·Ä∫·Çà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>·Äû·ÄΩ·ÄÑ·Ä∫·Å∂·ÇÉ·Åµ·Ä±·ÇÉ·Çà·Ä¢·Äô·Ä∫·Çá·Äû·Ä≠·ÄØ·Äï·Ä∫·Çá·Äú·Å¢·Äê·Ä∫·Çà·Äû·ÄÑ·Ä∫·Äë·ÇÖ·ÄÑ·Ä∫·Çà ·Äú·Ä≠·Ä∞·Äù·Ä∫·Äû·Ä±·Äï·Äù·Ä∫·Çá·Äï·ÄØ·Åº·Ä∫·Çà·Åµ·ÇÇ·Åº·Ä∫·Ä∏·Äú·ÇÖ·Åº·Ä∫·Ä∏·Äö·ÇÉ·Çà·Å∂·ÇÉ·Åµ·ÇÇ·ÇÉ·Çá ·ÇÅ·ÄÑ·Ä∫·Ä∏·ÅΩ·ÇÇ·Ä∫·ÇÅ·ÄÑ·Ä∫·Ä∏·Äô·Åº·Ä∫·Ä∏·Åµ·Ä∞·Å∫·Ä∫·Ä∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>·Äê·ÇÉ·Çá·Äô·ÇÉ·Ä∏·Å∏·Ä∞·Ä∏·Å∂·Äô·Ä∫·Ä∏·Äú·Ä≠·ÄØ·Åº·Ä∫·Ä∏·Äû·Ä± ·Åµ·ÇÑ·Çà·Å∂·ÇÜ·Äú·ÄΩ·ÄÑ·Ä∫·Çà·Åº·ÇÜ·Çâ·Åµ·ÇÇ·ÇÉ·Çá·ÇÅ·ÇÇ·Ä∫·Çà·Äö·Äù·Ä∫·Çâ·Äê·Ä∞·Äù·Ä∫·Çà·Åº·ÇÜ·Äö·Ä∞·Çá</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(shn_dataset_train.remove_columns([\"audio\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dcf75cb8cf4c3fba51233a562f115f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b609cac520c8434099f756acbce83025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\‚Äú\\%\\‚Äò\\‚Äù\\ÔøΩ\\'\\·Åã\\·Åä\\‚Ä¶]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"transcription\"] = re.sub(chars_to_remove_regex, '', batch[\"transcription\"]).lower()\n",
    "    return batch\n",
    "\n",
    "shn_dataset_train = shn_dataset_train.map(remove_special_characters)\n",
    "shn_dataset_test = shn_dataset_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·Äô·ÇÇ·Ä∫·Ä∏·ÇÅ·Ä∞·Çâ·Äú·ÄΩ·ÄÑ·Ä∫·Çà·Åµ·Äù·Ä∫·Åº·Äô·Ä∫·Åº·ÇÉ·Çá·Äö·Äù·Ä∫·Çâ·Åº·ÇÉ·Çà ·Ä¢·Åº·Ä∫·Äê·ÄÑ·Ä∫·Çà·Å∏·ÇÇ·Ä∫·Äú·Åµ·Ä∫·Çâ·Å∂·Ä∞·Äù·Ä∫·Å∂·ÄΩ·ÄÑ·Ä∫·Äï·Ä≠·Ä∞·Åº·Ä∫·Çà·Äû·Ä± ·Äê·ÄÑ·Ä∫·Çà·Å∏·ÇÇ·Ä∫·ÇÅ·ÇÇ·Ä∫·Çà·Äï·Ä≠·Ä∞·Åº·Ä∫·Çà·Å∫·ÄΩ·Äï·Ä∫·Ä∏·Äú·ÇÜ·Çà·Äê·Ä≠·Åµ·Ä∫·Ä∏·Äê·Ä≠·Åµ·Ä∫·Ä∏·Åº·ÇÜ·Çâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>·Åº·ÇÉ·Çâ·Äê·Ä±·Äú·Å¢·Äê·Ä∫·Çà·Åµ·ÇÇ·Å¢·Äô·Ä∫·Ä∏·Äú·Äï·Ä∫·Çâ·Åº·ÇÉ ·Äô·Ä≠·Ä∞·Äù·Ä∫·Çà·Å∏·Å¢·ÇÜ·Ä∏·ÇÅ·ÄΩ·ÄÑ·Ä∫·Çâ·ÅΩ·ÇÇ·Ä∫·Äù·ÇÉ·Çà·Ä¢·Ä∞·Çà ·Åµ·Ä±·ÇÉ·Çâ·Åº·Åº·Ä∫·Çâ·Åµ·Ä±·ÇÉ·Çà·Äï·Äµ·Åº·Ä∫·Ä¢·Ä∞·Çà·Å∏·Å¢·ÇÜ·Ä∏·Äö·Äù·Ä∫·Çâ·Åº·ÇÉ·Çà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·Åµ·Å¢·Åº·Ä∫·Äô·Åº·Ä∫·Ä∏·Åº·Åº·Ä∫·Çâ ·Ä¢·Äô·Ä∫·Çá·Äê·ÄΩ·Åº·Ä∫·Çâ·Äô·ÄÆ·Ä∏·Äô·Ä≠·Ä∞·Äù·Ä∫·Çà·Äö·Äù·Ä∫·Çâ·ÇÅ·Ä±·Äï·ÄΩ·Åµ·Ä∫·Çà·Äú·ÇÑ·Çà ·Äï·Ä±·ÇÉ·Ä∏·Ä¢·Åº·Ä∫·Åº·Ä≠·ÄØ·ÄÑ·Ä∫·Çà·Äê·Å¢·Åº·Ä∫·Çá·Äö·Äù·Ä∫·Çâ·Åµ·ÇÇ·ÇÉ·Çá ·Äë·ÇÖ·ÄÑ·Ä∫·Çà·Ä¢·Åº·Ä∫·Åµ·Ä±·ÇÉ·Çà·Äû·Äô·Ä∫·Çâ·Äï·Ä±·ÇÉ·Ä∏·Äô·ÇÉ·Ä∏·Äö·Ä∞·Çá·Äö·Äù·Ä∫·Çâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·Åº·ÇÇ·Ä∫·Ä∏·Å∂·Å¢·Äù·Ä∫·Ä∏·Äê·Å¢·ÄÑ·Ä∫·Ä∏·Äû·ÄΩ·ÄÑ·Ä∫·Äû·Å¢·Äô·Ä∫·Äï·ÄÆ·Äô·ÇÉ·Ä∏·Åº·ÇÜ·Çâ ·Å∏·Äù·Ä∫·Çà·Äú·ÄΩ·Åº·Ä∫·Çâ·Å∂·Ä≠·Ä∞·Äù·Ä∫·Ä∏·Ä¢·Äô·Ä∫·Çá·Äï·Ä±·ÇÉ·Ä∏·Äê·Åº·Ä∫·Ä∏·Äú·ÇÑ·Çà ·Ä¢·Äô·Ä∫·Çá·Äï·Ä±·ÇÉ·Ä∏·Äû·Ä∞·Çà·Äú·ÇÜ·Çà·Äô·ÇÉ·Ä∏·Å∏·ÇÇ·Ä∫·Ä∏·Å∂·Äù·Ä∫·Äô·ÇÑ·Çà·Äú·ÄØ·Åµ·Ä∫·Çà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>·Åº·ÇÇ·Ä∫·Ä∏·Äú·Ä≠·Åµ·Ä∫·Çà·Åº·Åº·Ä∫·Çâ·Äê·ÇÑ·Çâ ·Äú·ÇÜ·Çà·Äê·ÇÖ·Äô·Ä∫·Çà·Äë·Ä≠·ÄØ·ÄÑ·Ä∫ ·Å∂·Ä±·ÇÉ·Çà·Åµ·ÇÇ·Å¢·Äô·Ä∫·Ä∏·Åº·ÇÇ·Ä∫·Ä∏·Ä¢·Ä∞·Åµ·Ä∫·Ä∏·ÇÅ·Ä∞·Äù·Ä∫·Å∏·ÇÇ·Ä∫ ·Ä¢·Åº·Ä∫·Äú·ÇÜ·Çà·ÇÅ·Åµ·Ä∫·Çâ·Äô·ÇÖ·Åº·Ä∫·Çà·Äù·Åº·Ä∫·Ä∏·Å∏·Ä≠·ÄØ·Åº·Ä∫·Çà·Äú·Ä∞·Åº·Ä∫·Çâ·Äú·Ä≠·Ä∞·Äù·Ä∫·ÇÅ·ÇÖ·ÄÑ·Ä∫·Ä∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>·Ä¢·Å¢·ÇÜ·Çà·Å∏·Å¢·ÇÜ·Ä∏ ·Å∏·ÄΩ·ÄÑ·Ä∫·Çá·Äô·ÇÇ·Ä∫·Ä∏·ÇÅ·Ä∞·Çâ·ÇÅ·Ä≠·ÄØ·Äù·Ä∫·Äù·ÇÉ·Çà ·Äï·Ä±·ÇÉ·Ä∏·Äô·ÄÆ·Ä∏·Åº·ÇÉ·Çà·ÇÅ·Ä≠·Ä∞·Åº·Ä∫·Ä∏·Åº·ÇÜ ·Åº·ÇÉ·Çà·ÇÅ·Ä≠·Ä∞·Åº·Ä∫·Ä∏·Åº·Åº·Ä∫·Çâ ·Äï·Äµ·Åº·Ä∫·Äê·ÄÆ·Çà·Äö·Ä∞·Äï·Ä∫·Çà·Äö·ÄΩ·Äô·Ä∫·Ä∏·ÇÅ·Äù·Ä∫·Ä∏·Äô·ÇÉ·Ä∏·Åµ·Äô·Ä∫·Ä∏·Äú·Äµ·Äù·Ä∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>·Ä¢·Äô·Ä∫·Çá·Äï·Ä≠·Ä∞·ÄÑ·Ä∫·Çà·Äû·ÄÑ·Ä∫·Åµ·Åº·Ä∫·Äê·ÄÑ·Ä∫·Ä∏·Ä¢·Å¢·ÇÜ·Çà·Å∏·Ä≠·Ä∞·Äù·Ä∫·Ä∏·Åº·Åº·Ä∫·Çâ·Åº·ÄÑ·Ä∫·Çá·Åµ·Äù·Ä∫·Çá·Ä¢·Äô·Ä∫·Çá·Å∏·ÇÇ·Ä∫·Çà·ÇÅ·Ä≠·ÄØ·Äù·Ä∫ ·Äù·Ä∞·Åº·Ä∫·Çâ·Äù·ÇÉ·Çà·Ä¢·Åº·Ä∫·Äû·Ä∞·ÇÅ·Äµ·Äê·Ä∫·Ä∏·Åµ·ÇÇ·ÇÉ·Çá·Åº·Åº·Ä∫·Çâ ·Äô·Åº·Ä∫·Ä∏·Åµ·Ä≠·ÄØ·ÄÑ·Ä∫·Çá·Åµ·ÇÉ·Çà·Äô·Åº·Ä∫·Ä∏·Äö·Ä∞·Çá·ÇÅ·ÇÉ·Çâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>·ÇÅ·ÄΩ·ÄÑ·Ä∫·Çâ·Äú·Å¢·Äê·Ä∫·Çà·Å∏·Ä∞·Ä∏·Äù·Åº·Ä∫·Ä∏·Å∏·Ä≠·ÄØ·Åº·Ä∫·Çà·Åµ·ÇÇ·ÇÉ·Çá ·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·Äï·Å¢·Åº·Ä∫·Çá·Äö·Äù·Ä∫·Çâ·Å∂·ÇÉ·Çà ·Å∂·ÇÉ·Çà·Åµ·Ä∞·Äù·Ä∫·Äö·Äù·Ä∫·Çâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>·Äú·ÄΩ·ÄÑ·Ä∫·Çà·Åº·ÇÜ·Çâ·Åµ·Ä±·ÇÉ·Çà·Äê·Ä±·Å∏·Å¢·ÄÑ·Ä∫·Çà·Å∏·ÄΩ·ÇÜ·Çà·Äô·ÄΩ·Äê·Ä∫·Çá·Äï·ÇÖ·Äê·Ä∫·Çà ·Äú·ÄΩ·ÄÑ·Ä∫·Çà·Ä¢·Åº·Ä∫·Äô·Åº·Ä∫·Ä∏·Äú·ÇÜ·Çà·ÇÅ·Äµ·Äê·Ä∫·Ä∏·ÅΩ·Ä≠·Äê·Ä∫·Ä∏·Åµ·ÇÇ·ÇÉ·Çá·Äù·Åº·Ä∫·Ä∏·Åº·Åº·Ä∫·Çâ·Åº·ÇÜ·Äö·Ä∞·Çá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>·Ä¢·Ä±·ÇÉ·Çà ·Å∂·ÇÉ·Çà·Äê·Ä±·Ä¢·Äù·Ä∫·Åµ·ÇÇ·ÇÉ·Çá·Äï·Åº·Ä∫·Å∂·Äù·Ä∫·Å∂·Ä≠·ÄØ·Åº·Ä∫·Ä∏·Äê·ÄÑ·Ä∫·Ä∏·Äô·Ä∞·Äê·Ä∫·Ä∏ ·Ä¢·Åº·Ä∫·Å∂·ÇÉ·Çà·Äú·Åµ·Ä∫·Çâ·Äô·ÇÉ·Ä∏·Åº·Åº·Ä∫·Çâ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(shn_dataset_train.remove_columns([\"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3e362242da45a7a7e6c1c554f796c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4b1bb89b004f499deea40a71d41d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"transcription\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = shn_dataset_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=shn_dataset_train.column_names)\n",
    "vocab_test = shn_dataset_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=shn_dataset_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '\\xa0': 1,\n",
       " '·ÄÑ': 2,\n",
       " '·Äê': 3,\n",
       " '·Äë': 4,\n",
       " '·Äï': 5,\n",
       " '·Äô': 6,\n",
       " '·Äö': 7,\n",
       " '·Äú': 8,\n",
       " '·Äù': 9,\n",
       " '·Äû': 10,\n",
       " '·Ä¢': 11,\n",
       " '·Ä≠': 12,\n",
       " '·ÄÆ': 13,\n",
       " '·ÄØ': 14,\n",
       " '·Ä∞': 15,\n",
       " '·Ä±': 16,\n",
       " '·Äµ': 17,\n",
       " '·Ä∏': 18,\n",
       " '·Ä∫': 19,\n",
       " '·Äª': 20,\n",
       " '·Äº': 21,\n",
       " '·ÄΩ': 22,\n",
       " '·Å¢': 23,\n",
       " '·Åµ': 24,\n",
       " '·Å∂': 25,\n",
       " '·Å∏': 26,\n",
       " '·Å∫': 27,\n",
       " '·Åº': 28,\n",
       " '·ÅΩ': 29,\n",
       " '·Åæ': 30,\n",
       " '·ÇÅ': 31,\n",
       " '·ÇÇ': 32,\n",
       " '·ÇÉ': 33,\n",
       " '·ÇÑ': 34,\n",
       " '·ÇÖ': 35,\n",
       " '·ÇÜ': 36,\n",
       " '·Çá': 37,\n",
       " '·Çà': 38,\n",
       " '·Çâ': 39}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n",
    "\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lang = \"shn\"\n",
    "# new_vocab_dict = {target_lang: vocab_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "mms_adapter_repo = \"facebook/mms-1b-all\"\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(mms_adapter_repo)\n",
    "new_vocab = tokenizer.vocab\n",
    "\n",
    "new_vocab[target_lang] = vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w', encoding='utf-8') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"./models/wav2vec2-large-mms-1b-shan\"\n",
    "# tokenizer.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/wav2vec2-large-mms-1b-shan\\\\tokenizer_config.json',\n",
       " './models/wav2vec2-large-mms-1b-shan\\\\special_tokens_map.json',\n",
       " './models/wav2vec2-large-mms-1b-shan\\\\vocab.json',\n",
       " './models/wav2vec2-large-mms-1b-shan\\\\added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./models/wav2vec2-large-mms-1b-shan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'audio10.wav',\n",
       " 'array': array([-1.06073006e-09,  1.29704758e-09, -1.43051804e-09, ...,\n",
       "         7.50327745e-05,  6.82075042e-05,  0.00000000e+00]),\n",
       " 'sampling_rate': 22050}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shn_dataset_train[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "shn_dataset_train = shn_dataset_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "shn_dataset_test = shn_dataset_test.cast_column(\"audio\", Audio(sampling_rate=16_000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'audio10.wav',\n",
       " 'array': array([ 1.44173100e-08, -1.53559085e-08,  1.59961928e-08, ...,\n",
       "         3.89381676e-05,  7.58816605e-05,  0.00000000e+00]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shn_dataset_train[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text: ·Åµ·Ä∞·Å∫·Ä∫·Ä∏·Åµ·ÇÉ·Çà ·Äû·Ä∞·Äù·Ä∞·Åº·Ä∫·Çâ·Äù·ÇÉ·Çà ·Äï·Ä≠·Ä∞·Åº·Ä∫·Çà·Äê·Ä±·Äï·ÄΩ·ÇÜ·Çá·Äû·Ä∞·Äú·ÄΩ·Äê·Ä∫·Çà·Äú·ÇÜ·Çà·Äö·Ä∞·Çá·ÇÅ·ÇÉ·Çâ\n",
      "Input array shape: (73392,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(shn_dataset_train)-1)\n",
    "\n",
    "print(\"Target text:\", shn_dataset_train[rand_int][\"transcription\"])\n",
    "print(\"Input array shape:\", shn_dataset_train[rand_int][\"audio\"][\"array\"].shape)\n",
    "print(\"Sampling rate:\", shn_dataset_train[rand_int][\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prepair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"transcription\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0969ff838171485bb9242d2009abe800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293aefcd158c4ec085f5127a81120bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shn_dataset_train = shn_dataset_train.map(prepare_dataset, remove_columns=shn_dataset_train.column_names)\n",
    "shn_dataset_test = shn_dataset_test.map(prepare_dataset, remove_columns=shn_dataset_test.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.pad(\n",
    "            labels=label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mms-1b-all were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([44]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([44, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/mms-1b-all\",\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_adapter_layers()\n",
    "model.freeze_base_model()\n",
    "\n",
    "adapter_weights = model._get_adapters()\n",
    "for param in adapter_weights.values():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\transformers\\training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=32,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=200,\n",
    "  eval_steps=100,\n",
    "  logging_steps=5,\n",
    "  learning_rate=1e-3,\n",
    "  warmup_steps=100,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=shn_dataset_train,\n",
    "    eval_dataset=shn_dataset_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dc2ba9226242779517db89b5c64ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:963: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.5366, 'grad_norm': 22.734519958496094, 'learning_rate': 3e-05, 'epoch': 0.36}\n",
      "{'loss': 10.3854, 'grad_norm': 25.92425537109375, 'learning_rate': 8e-05, 'epoch': 0.71}\n",
      "{'loss': 9.2912, 'grad_norm': 26.86384391784668, 'learning_rate': 0.00013000000000000002, 'epoch': 1.07}\n",
      "{'loss': 7.9499, 'grad_norm': 30.73958396911621, 'learning_rate': 0.00017999999999999998, 'epoch': 1.43}\n",
      "{'loss': 6.3229, 'grad_norm': 28.897329330444336, 'learning_rate': 0.00023, 'epoch': 1.79}\n",
      "{'loss': 4.5586, 'grad_norm': 11.591548919677734, 'learning_rate': 0.00028000000000000003, 'epoch': 2.14}\n",
      "{'loss': 3.5702, 'grad_norm': 9.040257453918457, 'learning_rate': 0.00033, 'epoch': 2.5}\n",
      "{'loss': 3.4151, 'grad_norm': 3.3571383953094482, 'learning_rate': 0.00038, 'epoch': 2.86}\n",
      "{'loss': 3.1079, 'grad_norm': 4.979417324066162, 'learning_rate': 0.00043, 'epoch': 3.21}\n",
      "{'loss': 2.789, 'grad_norm': 2.4046664237976074, 'learning_rate': 0.00048, 'epoch': 3.57}\n",
      "{'loss': 2.223, 'grad_norm': 1.9828133583068848, 'learning_rate': 0.0005300000000000001, 'epoch': 3.93}\n",
      "{'loss': 1.5879, 'grad_norm': 1.2802170515060425, 'learning_rate': 0.00058, 'epoch': 4.29}\n",
      "{'loss': 1.0347, 'grad_norm': 1.0391294956207275, 'learning_rate': 0.00063, 'epoch': 4.64}\n",
      "{'loss': 0.7533, 'grad_norm': 2.2355473041534424, 'learning_rate': 0.00068, 'epoch': 5.0}\n",
      "{'loss': 0.5505, 'grad_norm': 0.9596748352050781, 'learning_rate': 0.00073, 'epoch': 5.36}\n",
      "{'loss': 0.4616, 'grad_norm': 0.7574542164802551, 'learning_rate': 0.0007800000000000001, 'epoch': 5.71}\n",
      "{'loss': 0.4132, 'grad_norm': 0.5114931464195251, 'learning_rate': 0.00083, 'epoch': 6.07}\n",
      "{'loss': 0.3628, 'grad_norm': 0.7413519620895386, 'learning_rate': 0.00088, 'epoch': 6.43}\n",
      "{'loss': 0.2802, 'grad_norm': 0.41764652729034424, 'learning_rate': 0.00093, 'epoch': 6.79}\n",
      "{'loss': 0.2918, 'grad_norm': 0.35124266147613525, 'learning_rate': 0.00098, 'epoch': 7.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0249bd5432ef4ac8844d004a2bef3113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2099316567182541, 'eval_wer': 0.5904761904761905, 'eval_runtime': 4.0181, 'eval_samples_per_second': 26.131, 'eval_steps_per_second': 3.484, 'epoch': 7.14}\n",
      "{'loss': 0.2726, 'grad_norm': 0.364082932472229, 'learning_rate': 0.000925, 'epoch': 7.5}\n",
      "{'loss': 0.2404, 'grad_norm': 0.496386855840683, 'learning_rate': 0.0008, 'epoch': 7.86}\n",
      "{'loss': 0.235, 'grad_norm': 0.614963173866272, 'learning_rate': 0.000675, 'epoch': 8.21}\n",
      "{'loss': 0.2115, 'grad_norm': 0.589288592338562, 'learning_rate': 0.00055, 'epoch': 8.57}\n",
      "{'loss': 0.2134, 'grad_norm': 0.5161349773406982, 'learning_rate': 0.000425, 'epoch': 8.93}\n",
      "{'loss': 0.2422, 'grad_norm': 0.6543455123901367, 'learning_rate': 0.0003, 'epoch': 9.29}\n",
      "{'loss': 0.204, 'grad_norm': 0.555431604385376, 'learning_rate': 0.000175, 'epoch': 9.64}\n",
      "{'loss': 0.218, 'grad_norm': 0.8375492095947266, 'learning_rate': 5e-05, 'epoch': 10.0}\n",
      "{'train_runtime': 505.0041, 'train_samples_per_second': 8.356, 'train_steps_per_second': 0.277, 'train_loss': 2.5615255756037576, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=2.5615255756037576, metrics={'train_runtime': 505.0041, 'train_samples_per_second': 8.356, 'train_steps_per_second': 0.277, 'total_flos': 2.8245716024730936e+18, 'train_loss': 2.5615255756037576, 'epoch': 10.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file as safe_save_file\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE\n",
    "import os\n",
    "\n",
    "adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(target_lang)\n",
    "adapter_file = os.path.join(training_args.output_dir, adapter_file)\n",
    "\n",
    "safe_save_file(model._get_adapters(), adapter_file, metadata={\"format\": \"pt\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models/wav2vec2-large-mms-1b-shan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttsmms_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
