{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install --upgrade pip \n",
    "%pip install datasets[audio]\n",
    "%pip install evaluate\n",
    "%pip install transformers\n",
    "%pip install jiwer\n",
    "%pip install accelerate\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "shn_dataset_train = load_dataset(\"norhsangpha/shn-asr-datasets\", split=\"train\", token=True)\n",
    "shn_dataset_test = load_dataset(\"norhsangpha/shn-asr-datasets\", split=\"test\", token=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "shn_dataset_train = load_from_disk(\"../../Datasets/mms-asr-nova-datasets-500/train\")\n",
    "shn_dataset_test = load_from_disk(\"../../Datasets/mms-asr-nova-datasets-500/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 422\n",
      "})\n",
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 105\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(shn_dataset_train)\n",
    "print(shn_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ၶဝ်ၵေႃႈ ဢွၼ်ၵၼ်ဢွၵ်ႇသုမ်ႉမႃးတူၺ်းသေ လႆႈႁၼ်ၽူႈမၢႆၼၼ်ႉ ၼွၼ်းလူမ်ႉတၢႆၵႂႃႇယဝ်ႉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ၶႃႈတေသိုပ်ႇဢဝ်ပုၼ်ႈၽွၼ်းၵႂႃႇၵူၺ်း လူဝ်ႇႁဵတ်းသင်ၵေႃႈဢုပ်ႇၵၼ်တင်းဝၼ်းၸိုၼ်ႈၵႂႃႇလႄႈၼႃႈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>သမ်ႉဝႃႈလွင်ႈမၼ်းပူၼ်ႉမႃးႁိုင်တၢၼ်ႇႁိုဝ်ၵေႃႈ လွင်ႈဝၼ်းလင်ၼၼ်ႉ ယင်းတိုၵ်ႉဢဝ်မႃးလၢတ်ႈထိုင်ၵၼ်လႆႈယူႇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ဝၼ်းၸိုၼ်ႈယၵ်ႉတူၺ်းၼႃႈၸၢႆးလိူၼ်ယဝ်ႉ ထူၺ်ႈၸႂ်ဢွၵ်ႇမႃးၼင်ႇလူမ်ၸႂ်</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ၵူၼ်းၼႆႉ မၼ်းတေမီးၽႂ်ဢၼ်ႁၼ်ငိုၼ်းယဝ်ႉ တေထဵင်လႆႈ ၼႂ်းၼၼ်ႉၵေႃႈယင်းပႃးၵူၼ်းၸိူင်ႉၼင်ႇၸဝ်ႈမိူင်းၶမ်းယူႇလူး</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ဝၼ်းၸိုၼ်ႈဢဝ်လႅၼ်းယႃႈမၼ်းထႅင်ႈလႅၼ်းၼိုင်ႈ ၸပ်းၾႆးယဝ်ႉ လုတ်ႇၵႂႃႇၵမ်းၼိုင်ႈသေလၢတ်ႈထႅင်ႈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>မိူဝ်ႈပိူၼ်ႈၵႂႃႇမူတ်းယဝ်ႉ ၸဝ်ႈလွၼ်ႉၶိူဝ်းၼၼ်ႉၵေႃႈ မႆႈၸႂ်မႆႈၶေႃး သမ်ႉပေႃးၵွတ်ႇၶမ်းလိုၼ်းၶဝ်ႈယူႇၼႂ်းဢူၵ်းမၼ်းၵႂႃႇယဝ်ႉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ပေႃးဝႃႈပဵၼ်ၵၢၼ်လႆႈငိုၼ်း သမ်ႉဝႃႈမၼ်းယၢပ်ႇၵေႃႈ ၵႃႈတေလႆႈၸုၼ်ႉႁေႁဵတ်းဢိူဝ်ႈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>သွင်ၶႃၵေႃႈဢမ်ႇသိုပ်ႇလၢတ်ႈသင်ထႅင်ႈ လိူဝ်သေပဝ်ႇပုၼ်ႈၵႂၼ်းလႅၼ်းယႃႈၶႃၵႂႃႇ ႁင်းၽႂ်ႁင်းမၼ်းၵူၺ်း</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>တႃႇမႃးၸူးၶမ်းလိုၼ်းသေ ၵႄႈၶႆလွင်ႈၼႆႉၵႂႃႇႁႂ်ႈယဝ်ႉတူဝ်ႈၼႆယူႇ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(shn_dataset_train.remove_columns([\"audio\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dcf75cb8cf4c3fba51233a562f115f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b609cac520c8434099f756acbce83025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\'\\။\\၊\\…]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"transcription\"] = re.sub(chars_to_remove_regex, '', batch[\"transcription\"]).lower()\n",
    "    return batch\n",
    "\n",
    "shn_dataset_train = shn_dataset_train.map(remove_special_characters)\n",
    "shn_dataset_test = shn_dataset_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>မႂ်းႁူႉလွင်ႈၵဝ်ၼမ်ၼႃႇယဝ်ႉၼႃႈ ဢၼ်တင်ႈၸႂ်လၵ်ႉၶူဝ်ၶွင်ပိူၼ်ႈသေ တင်ႈၸႂ်ႁႂ်ႈပိူၼ်ႈၺွပ်းလႆႈတိၵ်းတိၵ်းၼႆႉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ၼႃႉတေလၢတ်ႈၵႂၢမ်းလပ်ႉၼႃ မိူဝ်ႈၸၢႆးႁွင်ႉၽႂ်ဝႃႈဢူႈ ၵေႃႉၼၼ်ႉၵေႃႈပဵၼ်ဢူႈၸၢႆးယဝ်ႉၼႃႈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ၵၢၼ်မၼ်းၼၼ်ႉ ဢမ်ႇတွၼ်ႉမီးမိူဝ်ႈယဝ်ႉႁေပွၵ်ႈလႄႈ ပေႃးဢၼ်ၼိုင်ႈတၢၼ်ႇယဝ်ႉၵႂႃႇ ထႅင်ႈဢၼ်ၵေႃႈသမ်ႉပေႃးမႃးယူႇယဝ်ႉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ၼႂ်းၶၢဝ်းတၢင်းသွင်သၢမ်ပီမႃးၼႆႉ ၸဝ်ႈလွၼ်ႉၶိူဝ်းဢမ်ႇပေႃးတၼ်းလႄႈ ဢမ်ႇပေႃးသူႈလႆႈမႃးၸႂ်းၶဝ်မႄႈလုၵ်ႈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ၼႂ်းလိၵ်ႈၼၼ်ႉတႄႉ လႆႈတႅမ်ႈထိုင် ၶေႃႈၵႂၢမ်းၼႂ်းဢူၵ်းႁူဝ်ၸႂ် ဢၼ်လႆႈႁၵ်ႉမႅၼ်ႈဝၼ်းၸိုၼ်ႈလူၼ်ႉလိူဝ်ႁႅင်း</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ဢၢႆႈၸၢႆး ၸွင်ႇမႂ်းႁူႉႁိုဝ်ဝႃႈ ပေႃးမီးၼႃႈႁိူၼ်းၼႆ ၼႃႈႁိူၼ်းၼၼ်ႉ ပဵၼ်တီႈယူပ်ႈယွမ်းႁဝ်းမႃးၵမ်းလဵဝ်</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ဢမ်ႇပိူင်ႈသင်ၵၼ်တင်းဢၢႆႈၸိူဝ်းၼၼ်ႉၼင်ႇၵဝ်ႇဢမ်ႇၸႂ်ႈႁိုဝ် ဝူၼ်ႉဝႃႈဢၼ်သူႁဵတ်းၵႂႃႇၼၼ်ႉ မၼ်းၵိုင်ႇၵႃႈမၼ်းယူႇႁႃႉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ႁွင်ႉလၢတ်ႈၸူးဝၼ်းၸိုၼ်ႈၵႂႃႇ တွင်းပၢၼ်ႇယဝ်ႉၶႃႈ ၶႃႈၵူဝ်ယဝ်ႉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>လွင်ႈၼႆႉၵေႃႈတေၸၢင်ႈၸွႆႈမွတ်ႇပႅတ်ႈ လွင်ႈဢၼ်မၼ်းလႆႈႁဵတ်းၽိတ်းၵႂႃႇဝၼ်းၼၼ်ႉၼႆယူႇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ဢေႃႈ ၶႃႈတေဢဝ်ၵႂႃႇပၼ်ၶဝ်ၶိုၼ်းတင်းမူတ်း ဢၼ်ၶႃႈလၵ်ႉမႃးၼၼ်ႉ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(shn_dataset_train.remove_columns([\"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3e362242da45a7a7e6c1c554f796c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4b1bb89b004f499deea40a71d41d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"transcription\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = shn_dataset_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=shn_dataset_train.column_names)\n",
    "vocab_test = shn_dataset_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=shn_dataset_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '\\xa0': 1,\n",
       " 'င': 2,\n",
       " 'တ': 3,\n",
       " 'ထ': 4,\n",
       " 'ပ': 5,\n",
       " 'မ': 6,\n",
       " 'ယ': 7,\n",
       " 'လ': 8,\n",
       " 'ဝ': 9,\n",
       " 'သ': 10,\n",
       " 'ဢ': 11,\n",
       " 'ိ': 12,\n",
       " 'ီ': 13,\n",
       " 'ု': 14,\n",
       " 'ူ': 15,\n",
       " 'ေ': 16,\n",
       " 'ဵ': 17,\n",
       " 'း': 18,\n",
       " '်': 19,\n",
       " 'ျ': 20,\n",
       " 'ြ': 21,\n",
       " 'ွ': 22,\n",
       " 'ၢ': 23,\n",
       " 'ၵ': 24,\n",
       " 'ၶ': 25,\n",
       " 'ၸ': 26,\n",
       " 'ၺ': 27,\n",
       " 'ၼ': 28,\n",
       " 'ၽ': 29,\n",
       " 'ၾ': 30,\n",
       " 'ႁ': 31,\n",
       " 'ႂ': 32,\n",
       " 'ႃ': 33,\n",
       " 'ႄ': 34,\n",
       " 'ႅ': 35,\n",
       " 'ႆ': 36,\n",
       " 'ႇ': 37,\n",
       " 'ႈ': 38,\n",
       " 'ႉ': 39}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n",
    "\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lang = \"shn\"\n",
    "# new_vocab_dict = {target_lang: vocab_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "mms_adapter_repo = \"facebook/mms-1b-all\"\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(mms_adapter_repo)\n",
    "new_vocab = tokenizer.vocab\n",
    "\n",
    "new_vocab[target_lang] = vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w', encoding='utf-8') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"./models/wav2vec2-large-mms-1b-shan\"\n",
    "# tokenizer.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/wav2vec2-large-mms-1b-shan\\\\tokenizer_config.json',\n",
       " './models/wav2vec2-large-mms-1b-shan\\\\special_tokens_map.json',\n",
       " './models/wav2vec2-large-mms-1b-shan\\\\vocab.json',\n",
       " './models/wav2vec2-large-mms-1b-shan\\\\added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./models/wav2vec2-large-mms-1b-shan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'audio10.wav',\n",
       " 'array': array([-1.06073006e-09,  1.29704758e-09, -1.43051804e-09, ...,\n",
       "         7.50327745e-05,  6.82075042e-05,  0.00000000e+00]),\n",
       " 'sampling_rate': 22050}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shn_dataset_train[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "shn_dataset_train = shn_dataset_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "shn_dataset_test = shn_dataset_test.cast_column(\"audio\", Audio(sampling_rate=16_000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'audio10.wav',\n",
       " 'array': array([ 1.44173100e-08, -1.53559085e-08,  1.59961928e-08, ...,\n",
       "         3.89381676e-05,  7.58816605e-05,  0.00000000e+00]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shn_dataset_train[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text: ၵူၺ်းၵႃႈ သူဝူၼ်ႉဝႃႈ ပိူၼ်ႈတေပွႆႇသူလွတ်ႈလႆႈယူႇႁႃႉ\n",
      "Input array shape: (73392,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(shn_dataset_train)-1)\n",
    "\n",
    "print(\"Target text:\", shn_dataset_train[rand_int][\"transcription\"])\n",
    "print(\"Input array shape:\", shn_dataset_train[rand_int][\"audio\"][\"array\"].shape)\n",
    "print(\"Sampling rate:\", shn_dataset_train[rand_int][\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prepair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"transcription\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0969ff838171485bb9242d2009abe800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293aefcd158c4ec085f5127a81120bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shn_dataset_train = shn_dataset_train.map(prepare_dataset, remove_columns=shn_dataset_train.column_names)\n",
    "shn_dataset_test = shn_dataset_test.map(prepare_dataset, remove_columns=shn_dataset_test.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.pad(\n",
    "            labels=label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mms-1b-all were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([44]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([44, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/mms-1b-all\",\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_adapter_layers()\n",
    "model.freeze_base_model()\n",
    "\n",
    "adapter_weights = model._get_adapters()\n",
    "for param in adapter_weights.values():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\transformers\\training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=32,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=200,\n",
    "  eval_steps=100,\n",
    "  logging_steps=5,\n",
    "  learning_rate=1e-3,\n",
    "  warmup_steps=100,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=shn_dataset_train,\n",
    "    eval_dataset=shn_dataset_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dc2ba9226242779517db89b5c64ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:963: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.5366, 'grad_norm': 22.734519958496094, 'learning_rate': 3e-05, 'epoch': 0.36}\n",
      "{'loss': 10.3854, 'grad_norm': 25.92425537109375, 'learning_rate': 8e-05, 'epoch': 0.71}\n",
      "{'loss': 9.2912, 'grad_norm': 26.86384391784668, 'learning_rate': 0.00013000000000000002, 'epoch': 1.07}\n",
      "{'loss': 7.9499, 'grad_norm': 30.73958396911621, 'learning_rate': 0.00017999999999999998, 'epoch': 1.43}\n",
      "{'loss': 6.3229, 'grad_norm': 28.897329330444336, 'learning_rate': 0.00023, 'epoch': 1.79}\n",
      "{'loss': 4.5586, 'grad_norm': 11.591548919677734, 'learning_rate': 0.00028000000000000003, 'epoch': 2.14}\n",
      "{'loss': 3.5702, 'grad_norm': 9.040257453918457, 'learning_rate': 0.00033, 'epoch': 2.5}\n",
      "{'loss': 3.4151, 'grad_norm': 3.3571383953094482, 'learning_rate': 0.00038, 'epoch': 2.86}\n",
      "{'loss': 3.1079, 'grad_norm': 4.979417324066162, 'learning_rate': 0.00043, 'epoch': 3.21}\n",
      "{'loss': 2.789, 'grad_norm': 2.4046664237976074, 'learning_rate': 0.00048, 'epoch': 3.57}\n",
      "{'loss': 2.223, 'grad_norm': 1.9828133583068848, 'learning_rate': 0.0005300000000000001, 'epoch': 3.93}\n",
      "{'loss': 1.5879, 'grad_norm': 1.2802170515060425, 'learning_rate': 0.00058, 'epoch': 4.29}\n",
      "{'loss': 1.0347, 'grad_norm': 1.0391294956207275, 'learning_rate': 0.00063, 'epoch': 4.64}\n",
      "{'loss': 0.7533, 'grad_norm': 2.2355473041534424, 'learning_rate': 0.00068, 'epoch': 5.0}\n",
      "{'loss': 0.5505, 'grad_norm': 0.9596748352050781, 'learning_rate': 0.00073, 'epoch': 5.36}\n",
      "{'loss': 0.4616, 'grad_norm': 0.7574542164802551, 'learning_rate': 0.0007800000000000001, 'epoch': 5.71}\n",
      "{'loss': 0.4132, 'grad_norm': 0.5114931464195251, 'learning_rate': 0.00083, 'epoch': 6.07}\n",
      "{'loss': 0.3628, 'grad_norm': 0.7413519620895386, 'learning_rate': 0.00088, 'epoch': 6.43}\n",
      "{'loss': 0.2802, 'grad_norm': 0.41764652729034424, 'learning_rate': 0.00093, 'epoch': 6.79}\n",
      "{'loss': 0.2918, 'grad_norm': 0.35124266147613525, 'learning_rate': 0.00098, 'epoch': 7.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0249bd5432ef4ac8844d004a2bef3113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2099316567182541, 'eval_wer': 0.5904761904761905, 'eval_runtime': 4.0181, 'eval_samples_per_second': 26.131, 'eval_steps_per_second': 3.484, 'epoch': 7.14}\n",
      "{'loss': 0.2726, 'grad_norm': 0.364082932472229, 'learning_rate': 0.000925, 'epoch': 7.5}\n",
      "{'loss': 0.2404, 'grad_norm': 0.496386855840683, 'learning_rate': 0.0008, 'epoch': 7.86}\n",
      "{'loss': 0.235, 'grad_norm': 0.614963173866272, 'learning_rate': 0.000675, 'epoch': 8.21}\n",
      "{'loss': 0.2115, 'grad_norm': 0.589288592338562, 'learning_rate': 0.00055, 'epoch': 8.57}\n",
      "{'loss': 0.2134, 'grad_norm': 0.5161349773406982, 'learning_rate': 0.000425, 'epoch': 8.93}\n",
      "{'loss': 0.2422, 'grad_norm': 0.6543455123901367, 'learning_rate': 0.0003, 'epoch': 9.29}\n",
      "{'loss': 0.204, 'grad_norm': 0.555431604385376, 'learning_rate': 0.000175, 'epoch': 9.64}\n",
      "{'loss': 0.218, 'grad_norm': 0.8375492095947266, 'learning_rate': 5e-05, 'epoch': 10.0}\n",
      "{'train_runtime': 505.0041, 'train_samples_per_second': 8.356, 'train_steps_per_second': 0.277, 'train_loss': 2.5615255756037576, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=2.5615255756037576, metrics={'train_runtime': 505.0041, 'train_samples_per_second': 8.356, 'train_steps_per_second': 0.277, 'total_flos': 2.8245716024730936e+18, 'train_loss': 2.5615255756037576, 'epoch': 10.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file as safe_save_file\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE\n",
    "import os\n",
    "\n",
    "adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(target_lang)\n",
    "adapter_file = os.path.join(training_args.output_dir, adapter_file)\n",
    "\n",
    "safe_save_file(model._get_adapters(), adapter_file, metadata={\"format\": \"pt\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models/wav2vec2-large-mms-1b-shan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttsmms_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
