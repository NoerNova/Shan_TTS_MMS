{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at facebook/mms-1b-all were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
    "\n",
    "finetune_model_id = \"./models/wav2vec2-large-mms-1b-shan\"\n",
    "original_model_id = \"facebook/mms-1b-all\"\n",
    "\n",
    "finetune_model = Wav2Vec2ForCTC.from_pretrained(finetune_model_id, target_lang=\"shn\", ignore_mismatched_sizes=True).to(\"cuda\")\n",
    "finetune_processor = AutoProcessor.from_pretrained(finetune_model_id)\n",
    "\n",
    "original_model = Wav2Vec2ForCTC.from_pretrained(original_model_id).to(\"cuda\")\n",
    "original_processor = AutoProcessor.from_pretrained(original_model_id)\n",
    "original_processor.tokenizer.set_target_lang(\"shn\")\n",
    "original_model.load_adapter(\"shn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio, load_dataset\n",
    "\n",
    "test_data = load_dataset(\"NorHsangPha/shn-asr-datasets\", split=\"test\", token=True)\n",
    "test_data = test_data.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'audio35.wav', 'array': array([-0.0038032 , -0.00612239, -0.00519261, ..., -0.00181367,\n",
      "       -0.00168416, -0.00077092]), 'sampling_rate': 16000}\n",
      "သိူဝ်ႈႁတ်ႉၶႅၼ်ပွတ်းသီၶဵဝ်လိူင် ထႅင်ႈပၼ်တၢင်းႁၢင်ႈလီၸွမ်းၽိဝ်ၼိူဝ်ႉၼင်ၶၢဝ်လိူင်ဢွၼ်ႇမၼ်းၼၢင်းၼၼ်ႉထူၼ်ႈ\n"
     ]
    }
   ],
   "source": [
    "print(test_data[20][\"audio\"])\n",
    "print(test_data[20][\"transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data_sample = test_data[20]\n",
    "audio_samples = selected_data_sample[\"audio\"][\"array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "ASR_SAMPLING_RATE = 16_000\n",
    "sample_num = 301\n",
    "audio_fp = f\"../../Datasets/dataset_2/audio-data/train/audio{sample_num}.wav\"\n",
    "df = pd.read_csv(\"../../Datasets/dataset_2/metadata.csv\")\n",
    "\n",
    "\n",
    "audio_samples = librosa.load(audio_fp, sr=ASR_SAMPLING_RATE, mono=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NoerN\\.conda\\envs\\mmslabs\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "finetune_input_dict = finetune_processor(audio_samples, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "finetune_logits = finetune_model(finetune_input_dict.input_values.to(\"cuda\")).logits\n",
    "finetune_pred_ids = torch.argmax(finetune_logits, dim=-1)[0]\n",
    "\n",
    "original_input_dict = original_processor(audio_samples, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "original_logits = original_model(original_input_dict.input_values.to(\"cuda\")).logits\n",
    "original_pred_ids = torch.argmax(original_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:orange'>\n",
       "Finetune model Prediction:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ၼႂ်းၶၢ်းတၢင်းသိပ်းပီဢၼ်ဢမ်ႇလႆႈႁူပ်ႉၺႃးၵၼ်ၼၼ်ႉၵေႃႈ မၼ်းပဵၼ်လွင်ႈတၢင်ႈၸႂ်ႁဝ်းမိူၼ်ၵၼ်\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:yellow'>\n",
       "Original model Prediction:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ၼႂ်ႉ ၶႃး တၢင်း သိပ်ႉ ပီ ဢၼ် ဢမ်ႇ လႆႈ ႁွပ်ႉၺႃး ၵၼ် ၼၼ်ႉ ၵေႃႈ မၼ်း ပိူၼ်ႇ လွင်ႈ တၢင်ႈၸႂ် ႁဝ်း မိူၼ်ႇ ၵၼ် -\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:green'>\n",
       "Reference:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ၼႂ်းၶၢဝ်းတၢင်းသိပ်းပီ ဢၼ်ဢမ်ႇလႆႈႁူပ်ႉၺႃးၵၼ်လႆႈၼၼ်ႉၵေႃႈ မၼ်းပဵၼ်လွင်ႈတင်ႈၸႂ်ႁဝ်းမိူၼ်ၵၼ်\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "\n",
    "\n",
    "printmd(\"\\nFinetune model Prediction:\", color=\"orange\")\n",
    "print(finetune_processor.decode(finetune_pred_ids))\n",
    "\n",
    "printmd(\"\\nOriginal model Prediction:\", color=\"yellow\")\n",
    "print(original_processor.decode(original_pred_ids))\n",
    "\n",
    "printmd(\"\\nReference:\", color=\"green\")\n",
    "# print(selected_data_sample[\"transcription\"])\n",
    "print(df[\"transcription\"][sample_num-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(finetune_model_id)\n",
    "\n",
    "token = tokenizer(\"ၾႃႉၾူၼ်ၵမ်ႇလမ်သႃး\")\n",
    "\n",
    "tokenizer.decode(token[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.push_to_hub(\"haohaa/wav2vec2-large-mms-1b-shan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmslabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
